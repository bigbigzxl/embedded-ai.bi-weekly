---
layout: default
---

# 嵌入式AI简报 (2020-06-17)

**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：本次16条。

首先是端侧芯片方面的新闻速览：

- 华为海思招聘全球天才少年，包括2017年1月1日至2021年12月31日期间毕业的全球优秀本科生、硕士生、博士生，挑战课题涉及计算机体系结构类、芯片类、算法类、半导体工艺类、材料应用类、软件架构类；  
- [ARM与华为的合作关系正式终止。虽后者已获ARMv8指令集授权，后续也可基于该指令集研发新一代的手机处理器，但对ARM未来新推出的CPU内核IP等新指令集架构授权，包括新Mali系列GPU和多媒体IP授权也无法获得](https://mp.weixin.qq.com/s/npIsOUJR-bph0BAEMrOboA)；



海思表示：“我们一直在寻找芯星发光的人才，寻找新世界勇于挑战的勇敢者，一起胸怀世界、洞见新知。面对摩尔定律极限、冯诺依曼瓶颈、香农极限等，希望有同行者一起探索终将面临的世界级难题，找到领先世界的计算架构的方法和路径，共同拓展文明进步的边界。”


> 注：个别链接打不开，请点击文末阅读原文跳转

## 业界新闻


- [Intel Exposes OpenCL: 3.0 For Tiger Lake Graphics]

- [风口浪尖上的Arm China，高管团队力挺CEO | 半导体行业观察](https://mp.weixin.qq.com/s/E6hg_hIj6t6BlKSYHNTCLw)  
摘要：安谋中国（Arm China）和安谋公司（Arm Limited）围绕吴雄昂先生的“争论”引起了行业人士的高度关注。昨日，安谋中国各部门高管共同署名发布了《星辰山海不负韶华 凝芯聚力砥砺前行》的联名信，在联名信中高管团队表示，作为公司管理团队，绝大部分都是与Allen（Allen Wu，吴雄昂英文名）共事十年以上、彼此深入了解的同事，他们对于近期媒体报道中对Allen的莫须有的指控感到非常震惊和气愤。  
在短短几天时间，四份声明，一封联名信，三次反转，为什么关于CEO的去留始终没有定论？Allen又是如何赢得团队如此的信任？安谋中国这两年究竟取得了什么成绩？我们或许能从这封联名信上获知一二。  


- [Cortex-X1+A78，规格数据惊人！AMD将搅局移动处理器 | EETOP](https://mp.weixin.qq.com/s/_WDfFjjCBkCq-J8HpH6A_g)  
摘要：AMD 将有望发布一款全新的移动 SoC 芯片：AMD Ryzen C7。  
曝光信息显示，这款芯片将配备两个基于3.0Ghz Cortex-X1 的 Gaugin_Pro mobile 内核、两个基于2.6Ghz Cortex-A78 的 Gaugin_Mobile 内核、四个2.0Ghz Cortex-A55 内核、AMD Radeon RDNA 2 移动 GPU以及联发科 5G UltraSave 调制解调器。曝光信息还透露 Ryzen C7 将与联发科携手合作，引入最新的 MTK 5G 基带。  
AMD Radeon RDNA 2 移动 GPU包括 4 个 CU，最高 700 Mhz，支持光线跟踪、可变速率着色技术、2K × 144 Hz 显示技术以及 HDR10 + 技术，综合性能超骁龙865的 Adreno 650 约 45% 左右。  


## 论文

- [ICLR2020] [MIT韩松等人提出新型Lite Transformer：模型压缩95% | 机器之心](https://mp.weixin.qq.com/s/RZ0wnMNfLvQGobj0kq82_w)  
标题：Lite Transformer with Long-Short Range Attention  
链接：https://arxiv.org/abs/2004.11886v1  
摘要：MIT 与上海交大的研究人员提出了一种高效的移动端 NLP 架构 Lite Transformer，向在边缘设备上部署移动级 NLP 应用迈进了一大步。该论文已被人工智能顶会 ICLR 2020 收录。  
其核心是长短距离注意力（Long-Short Range Attention，LSRA），该架构分别捕获局部和全局上下文。其中一组注意力头（通过卷积）负责局部上下文建模，而另一组则（依靠注意力）执行长距离关系建模。最后为了进一步减少计算量，普通卷积被替换为由线性层和深度卷积。  
- [ICLR2020满分] [为什么梯度裁剪能加速模型训练 | 夕小瑶的卖萌屋](https://mp.weixin.qq.com/s/--8s2fA80Md82MqcrwOsKw)  
标题：Why gradient clipping accelerates training: A theoretical justification for adaptivity  
链接：https://arxiv.org/abs/1905.11881  
摘要：本文简要介绍了ICLR2020的一篇分析梯度裁剪的满分论文，主要思路是引入了更宽松普适的假设条件，在新的条件下能体现出了梯度裁剪的必要性，并且由于放松了传统的约束，因此理论结果的适用范围更广，这也就表明，梯度裁剪确实是很多场景下都适用的技巧之一。  
- [CVPR2020] [动态卷积：自适应调整卷积参数，显著提升模型表达能力 | 微软研究院AI头条](https://mp.weixin.qq.com/s/eRZ3jNuceMYKE3lEj-g1aw)  
标题：Dynamic Convolution: Attention over Convolution Kernels  
链接：https://arxiv.org/abs/1912.03458  
摘要：轻量级卷积神经网络能够在较低的计算预算下运行，却也牺牲了模型性能和表达能力。对此，微软 AI 认知服务团队提出了动态卷积，与传统的静态卷积（每层单个卷积核）相比，根据注意力动态叠加多个卷积核不仅显著提升了表达能力，额外的计算成本也很小，因而对高效的 CNN 更加友好。  
动态卷积没有在每层上使用单个卷积核，而是根据注意力动态地聚合多个并行卷积核。注意力会根据输入动态地调整每个卷积核的权重，从而生成自适应的动态卷积。由于注意力是输入的函数，动态卷积不再是一个线性函数。通过注意力以非线性方式叠加卷积核具有更强的表示能力。  
动态网络引入了两部分额外计算：注意力模型和卷积核的叠加。注意力模型计算复杂度很低，由 avg pool 和两层全卷积组成。得益于小的内核尺寸，叠加多个卷积核在计算上也非常高效。因此，动态卷积引入的额外计算是非常少的。少量的额外计算与显著的表达能力的提升使得动态卷积非常适合轻量级的神经网络。  
本文提出，解决这个问题有两个关键点。首先，限制注意力的取值将简化注意力模型的学习。注意力取值的限制将缩小多个卷积的叠加核的取值空间。文中将注意力取值限制在0与1之间，同时所有注意力的和为1。如图3所示，如果使用3个卷积核，注意力在0与1之间把叠加核限制在两个三棱锥中，注意力的和为1把叠加核进一步限制在以这三个卷积核为顶点的三角形中。对于这两个限制，softmax 是一个很自然的选择。  
- [预训练语言模型通用压缩方法MiniLM助你模型小快好 | 微软研究院AI头条](https://mp.weixin.qq.com/s/CkAHKXWi24tDBz4HiWkhBw)  
标题：MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers  
链接：https://arxiv.org/abs/2002.10957  
摘要：本文介绍了基于 Transformer 预训练语言模型的通用压缩方法：深度自注意力知识蒸馏（Deep Self-Attention Distillation），深度迁移“老师”模型最后一层自注意力知识。  
该压缩方法核心思想是迁移大模型自注意力知识，以让小模型自注意力模块的行为和大模型尽可能相似。但只迁移了大模型最后一层的自注意力知识。因为如果每一层都进行知识迁移，那么就需要手动设计大小模型间的层对应关系来完成迁移。相较而言，只迁移最后一层知识简单有效，训练速度更快，而且不需要手动设计大小模型的层对应关系。  
深度自注意力知识蒸馏的方法，主要由两种知识迁移构成，第一种就是自注意力得分/分布迁移（Self-Attention Distribution Transfer），主要迁移自注意力得分/分布知识（Attention Scores/Distributions）。自注意力得分矩阵由Queries 和 Keys 通过点积操作得到，矩阵中每个值表示两个词的依赖关系。自注意力得分矩阵是自注意力模块中至关重要的知识，通过相对熵（KL-Divergence）来计算大模型和小模型自注意力得分矩阵的差异。  
该压缩方法简单有效，由不同预训练大模型压缩得到的英文和多语言 MiniLM 预训练模型更小更快，在自然语言理解和生成任务上均取得了出色的结果。  
- [Facebook最新力作FBNetV3来了！相比ResNeSt提速5倍，精度不输EfficientNet | 极市平台](https://mp.weixin.qq.com/s/y7v-hcxdqIFwgKU2nS6lSA)  
标题：FBNetV3: Joint Architecture-Recipe Search using Neural Acquisition Function  
链接：https://arxiv.org/abs/2006.02049  
摘要：这篇论文提到了一个比较有意思的点：网络架构与训练策略同时进行搜索。这是之前的方法所并未尝试的一个点，之前的方法主要聚焦在网络架构，而训练方法则是采用比较常规的一组训练方式。也许这就是“灯下黑”的缘故吧，看到了网络架构的重要性影响而忽略了训练方式在精度方面的“小影响”。但是，当精度达到一定程度之后，训练方式的这点影响就变得尤为重要了。  
所以Facebook的研究员从这点出发提出了FBNetV3，它将网络架构与对应的训练策略通过NAS联合搜索。在ImageNet数据集上，FBNetV3取得了媲美EfficientNet与ResNeSt性能的同时具有更低的FLOPs(1.4x and 5.0x fewer)；更重要的是，该方案可以跨网络、跨任务取得一致性的性能提升。  
Facebook的研究员提出了一种比较好的NAS方案，它将训练策略纳入到网络架构的搜索过程中。之前的研究往往只关注网络架构的搜索而忽视了训练策略的影响，这确实是一点比较容易忽视的。所谓的“灯下黑”，吼吼，笔者突然意识到：是不是还可以将参数初始化方式那纳入到搜索空间呢？感兴趣的小伙伴还不快点去尝试一下。  


## 开源项目

> 注：每条内容前缀为github地址的仓库拥有者和仓库名，补全地址后为`github.com/<repo_owner>/<repo_name>`。

- [Tencent/TNN：腾讯优图开源深度学习推理框架TNN](https://github.com/Tencent/TNN)  
摘要：6月10日，腾讯优图实验室宣布正式开源新一代移动端深度学习推理框架 TNN。支持主流安卓、iOS 等操作系统，适配 CPU、 GPU、NPU 硬件平台。通过 ONNX 可支持 TensorFlow、PyTorch、MXNet、Caffe 等多种训练框架。  
通过 GPU 深度调优、ARM SIMD 深入汇编指令调优、低精度计算等技术手段，TNN 在性能上取得了进一步提升。引入了 INT8、 FP16、 BFP16 等多种计算低精度的支持。通过采用 8bit 整数代替 float 进行计算和存储，使模型尺寸和内存消耗均减少至 1/4，计算性能提升 50% 以上。  
此外，TNN 还引入 arm 平台 BFP16 的支持。相比浮点模型，BFP16 使模型尺寸、内存消耗减少 50%，在中低端机上的性能提升约 20%。  
- [Uber/Neuropod：Uber开源深度学习推理引擎Neuropod：支持调用TensorFlow、PyTorch等框架 | AI前线](https://mp.weixin.qq.com/s/2TVvbunzAPzdqyluruFaQw)  
摘要：Neuropod 是现有深度学习框架之上的一个抽象层，它提供了一个统一的接口来运行深度学习模型。Neuropod 让研究人员可以轻松地在自己选择的框架中构建模型，同时也简化了这些模型的生产化过程。  
Neuropod 支持的框架包括：TensorFlow、PyTorch、Keras 和 TorchScript，同时也可以轻松地添加新的框架。  
通过 Neuropod，应用程序只与与框架无关的 API 进行交互，并且 Neuropod 将这些与框架无关的调用转换为对底层框架的调用，调用过程尽可能使用零拷贝操作来高效地实现。  
- [NVIDIA Jetson强力工具jtop：比htop更强 | 吉浦迅科技](https://mp.weixin.qq.com/s/Rrmk-SteZnJ0M5s2fChq7w)  
摘要：这个不算是开源是一个小工具。通过jtop工具，可以查看Jetpack版本，Xavier工作模式，CPU数量，风扇状态，RAM，GPU状态频率。工具安装简单pip3 install jetson-stats，直接sudo jtop来执行就能查看各种信息，结合-h参数查看更多使用方法。  


## 博文


- [渐入佳境的ARM：从A78浅谈ARM架构的发展 | NeuralTalk](https://mp.weixin.qq.com/s/349MAZWE4cvqt1p6q9kccw)  
摘要：作者MikeslCroom是一位十年芯片老兵，本文将对ARM的移动端新一代核心A78、X1，对其架构的发展历程做一个分析。  
- [Arm Mali GPU 四大微架构概述 | NeuralTalk](https://mp.weixin.qq.com/s/Iv-__XF6z3EtUYIka4sKAg)  
摘要：本文翻译自ARM官方的GPU Architecture页面，并结合作者的经验写就而成，简单介绍了Mali的GPU的四个架构系列（Utgard、Midgard、Bifrost、Valhall）的特点。  
- [从英伟达A100 GPU说起，浅谈细粒度结构化稀疏 | 机器之心](https://mp.weixin.qq.com/s/bg8rdl8W-wohLDMYMRJlYA)  
摘要：本文要讲的是模型轻量化方法中的稀疏化等做法。以往单个权重的剪枝是随机的，导致中间很多0访存不连续，后来有了针对权重向量、单个卷积通道，整个卷积核的卷积。虽然这种结构化稀疏提升了推理速度，但是牺牲了模型精度，而原始的细粒度剪枝在推理速度上提升效果不佳。  
英伟达提出，对网络的稀疏化过程首先将权重分组，这里所举的示例以 4 个相邻的权重为一组。而后在每个权重组内限定固定个数的非零权重数目对权重进行稀疏化。在稀疏化处理后，每个权重组（同一颜色块）中都正好留下 2 个非零权重，也就是 50% 的稀疏度。  
通过对模型施加这一结构约束，获得了规整的结构，而且随机性被限制在了每个组内。随机性带来的问题可通过将同一组权重对应的数据完全装载到计算核心的寄存器上来解决，以避开随机访问外存的延迟。这一结构也允许对网络推理计算在组边界上做分割, 从而将计算分块化以最大程度利用的加速器的多核并行计算能力。  
作者建议首先训练非稀疏网络，而后对网络进行细粒度结构化剪枝，再使用学习率重卷 (learning rate rewinding) 的方式对网络进行重训练。这里的 learning rate rewinding 就是对剪枝后的网络使用原始网络的学习率安排表进行重训练。细粒度结构化稀疏的研究应用目前还处在较初期的阶段，由于其独特的硬件友好特性相信这一设计未来会被更多的加速器采用。  
- [X86 Intel CPU上的小半径中值滤波的极速优化 | GiantPandaCV](https://mp.weixin.qq.com/s/nQ-CevrSkVj4kyS3q360VA)  
摘要：本文以一个的中值滤波作为切入点，讨论了一下针对这个具体问题的优化思路，从算法逻辑到SSE、再到AVX等，速度也从最开始普通实现的8293.79ms优化到了9.32ms，具有一定参考意义。  
- [34页PPT全解嵌入式AI框架Tengine的架构、算子定制和引擎推理 | 智东西公开课](https://mp.weixin.qq.com/s/NiJVuIokVrCWfnJv7pxklw)  
摘要：本文全面解析Tengine架构及推理API的组成。最后，手把手教你如何通过Tengine定制个性化算子及在CPU/GPU/NPU/DLA上的推理。主要分为以下几个部分：嵌入式AI的挑战和Tengine的解决方案、Tengine架构解析、Tengine API简介，最后是2个实践部分：定制和添加Tengine算子和在CPU/GPU/NPU/DLA上做推理的实操。

## [往期回顾](https://github.com/ysh329/awesome-embedded-ai)


| 2 | 0 | 2 | 0 |
|:---:|:---:|:---:|:---:|
|  | [2020-06-03](../embedded-ai-report/2020-06-03.md)  | [2020-05-15](../embedded-ai-report/2020-05-15.md) | [2020-04-26](../embedded-ai-report/2020-04-26.md) |  
| [2020-04-04](../embedded-ai-report/2020-04-04.md) | [2020-03-19](../embedded-ai-report/2020-03-19.md) | [2020-03-02](../embedded-ai-report/2020-03-02.md) | [2020-02-16](../embedded-ai-report/2020-02-16.md) |  
| [2020-01-27](../embedded-ai-report/2020-01-27.md) | [2020-01-06](../embedded-ai-report/2020-01-06.md) | [2019-12-17](../embedded-ai-report/2019-12-17.md)  |  [2019-12-02](../embedded-ai-report/2019-12-02.md) |
| 2 | 0 | 1 | 9 |  
| [2019-11-30](../embedded-ai-report/2019-11-30.md) | [2019-11-18](../embedded-ai-report/2019-11-18.md) | [2019-10-31](../embedded-ai-report/2019-10-31.md)  |  [2019-10-17](../embedded-ai-report/2019-10-17.md) |  
| [2019-10-03](../embedded-ai-report/2019-10-03.md) | [2019-09-16](../embedded-ai-report/2019-09-16.md) | [2019-08-30](../embedded-ai-report/2019-08-30.md)  |  [2019-08-15](../embedded-ai-report/2019-08-15.md) |  
| [2019-07-30](../embedded-ai-report/2019-07-30.md) | [2019-07-15](../embedded-ai-report/2019-07-15.md) | [2019-06-29](../embedded-ai-report/2019-06-29.md)  |  [2019-06-17](../embedded-ai-report/2019-06-17.md) |  
| [2019-05-30](../embedded-ai-report/2019-05-30.md) | [2019-05-15](../embedded-ai-report/2019-05-15.md) | [2019-04-27](../embedded-ai-report/2019-04-27.md)  |  [2019-04-13](../embedded-ai-report/2019-04-13.md) |  
| [2019-03-31](../embedded-ai-report/2019-03-31.md) | | |  

----

> 注：个别链接打不开，请点击文末阅读原文跳转

![wechat_qrcode](../wechat_qrcode.jpg)

> 往期回顾：见公众号主菜单【历史消息】
- WeChat: NeuralTalk  
- Editor: https://github.com/ysh329  
- Project: https://github.com/ysh329/awesome-embedded-ai  

----

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">知识共享署名-相同方式共享 4.0 通用许可协议</a>进行许可。
