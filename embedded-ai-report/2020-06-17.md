---
layout: default
---

# 嵌入式AI简报 (2020-06-17)

**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：本次16条。【业界新闻】TensorFlow Lite亮相TF DevSummit性能方面支持OpenCL且CPU基于ruy性能也有提升，微控制器方面持续发力等等；ARM Cortex-A78和旗舰X1的新架构发布，树莓派4发布；【论文】谷歌提出数据回波榨干训练时的GPU资源、CVPR2020不引入约束的剪枝方法，以及BERT压缩方面的DynaBERT。【开源】本月可能会有2家推理框架开源。华为Mindspore发布新版本支持二阶优化，训练速度快到爆炸、端上手写体训练项目MNIST-CoreML-Training；【博文】Tengine支持armv8.2的cpu fp16特性加速推理，且支持加载NCNN模型；一篇对TFLite的Delegate的科普；Face++在端侧框架cpu方面所做的工作总结；TVM在Web上对WebAssembly和WebGPU的尝试；探秘嵌入式系统内存管理机制。

首先是端侧芯片方面的新闻，开胃：

- 去年，华为就开始将更多的中低端移动芯片项目分配给联发科。今年，华为已经成为联发科中端5G移动芯片的关键客户之一；  
- [三星5nm Exynos SoC 今年8月即将量产：性能大涨](https://mp.weixin.qq.com/s/l-0ygc3jmcOFXDbLdEWxPg)，且会使用Cortex A78和Mali G78等公版架构来提升性能，毕竟自研团队已经解散；
- [联发科这次5G SoC虽然没有主频高到2.8的单个核心，但其长期高负载使用中，还能保持步调一致的相对高频，超过对手剩下的中小核心](https://mp.weixin.qq.com/s/OGj2cnfxpReoo0skO13pjg)；  
- 索尼宣布即将发布两款智能视觉传感器：IMX500和IMX501，这是全球首款配备人工智能（AI）处理功能的CMOS图像传感器，且集成AI处理功能，可实现高速边缘AI处理；  
- [OPPO首次公开了关于自研芯片的“马里亚纳计划”](https://mp.weixin.qq.com/s/D_SCX0sEObSsWp1JoCHU4A)，从供应商方面如紫光展锐和联发科方面招募了不少工程师等高管；
- 神盾与抖音联合研发的AI芯片，采用台积电的7nm制程，用于影像处理等方面，量产时间落在2021年初。抖音方面回应：消息不实。

> 注：个别链接打不开，请点击文末阅读原文跳转

## 业界新闻


- [风口浪尖上的Arm China，高管团队力挺CEO
半导体行业观察](https://mp.weixin.qq.com/s/E6hg_hIj6t6BlKSYHNTCLw)  
摘要：安谋中国（Arm China）和安谋公司（Arm Limited）围绕吴雄昂先生的“争论”引起了行业人士的高度关注。昨日，安谋中国各部门高管共同署名发布了《星辰山海不负韶华 凝芯聚力砥砺前行》的联名信，在联名信中高管团队表示，作为公司管理团队，绝大部分都是与Allen（Allen Wu，吴雄昂英文名）共事十年以上、彼此深入了解的同事，他们对于近期媒体报道中对Allen的莫须有的指控感到非常震惊和气愤。  
在短短几天时间，四份声明，一封联名信，三次反转，为什么关于CEO的去留始终没有定论？Allen又是如何赢得团队如此的信任？安谋中国这两年究竟取得了什么成绩？我们或许能从这封联名信上获知一二。  
- [突发：ARM正式断供华为 | 半导体行业联盟](https://mp.weixin.qq.com/s/npIsOUJR-bph0BAEMrOboA)  
摘要：ARM与华为的合作关系正式终止。虽然华为获得了ARMv8指令集的授权，华为后续也完全可以基于ARMv8指令集研发新一代的手机处理器，但是无法获取ARM未来新推出的CPU内核IP或者新的指令集架构授权，包括新的Mali系列GPU和多媒体IP授权也无法获得。  
- [Cortex-X1+A78，规格数据惊人！AMD将搅局移动处理器 | EETOP](https://mp.weixin.qq.com/s/_WDfFjjCBkCq-J8HpH6A_g)  
摘要：AMD 将有望发布一款全新的移动 SoC 芯片：AMD Ryzen C7。  
曝光信息显示，这款芯片将配备两个基于3.0Ghz Cortex-X1 的 Gaugin_Pro mobile 内核、两个基于2.6Ghz Cortex-A78 的 Gaugin_Mobile 内核、四个2.0Ghz Cortex-A55 内核、AMD Radeon RDNA 2 移动 GPU以及联发科 5G UltraSave 调制解调器。曝光信息还透露 Ryzen C7 将与联发科携手合作，引入最新的 MTK 5G 基带。  
AMD Radeon RDNA 2 移动 GPU包括 4 个 CU，最高 700 Mhz，支持光线跟踪、可变速率着色技术、2K × 144 Hz 显示技术以及 HDR10 + 技术，综合性能超骁龙865的 Adreno 650 约 45% 左右。  




## 论文

- [ICLR2020] [MIT韩松等人提出新型Lite Transformer：模型压缩95% | 机器之心](https://mp.weixin.qq.com/s/RZ0wnMNfLvQGobj0kq82_w)  
标题：Lite Transformer with Long-Short Range Attention  
摘要：MIT 与上海交大的研究人员提出了一种高效的移动端 NLP 架构 Lite Transformer，向在边缘设备上部署移动级 NLP 应用迈进了一大步。该论文已被人工智能顶会 ICLR 2020 收录。  
其核心是长短距离注意力（Long-Short Range Attention，LSRA），该架构分别捕获局部和全局上下文。其中一组注意力头（通过卷积）负责局部上下文建模，而另一组则（依靠注意力）执行长距离关系建模。最后为了进一步减少计算量，普通卷积被替换为由线性层和深度卷积。  
- [ICLR2020] [为什么梯度裁剪能加速模型训练 | 夕小瑶的卖萌屋](https://mp.weixin.qq.com/s/--8s2fA80Md82MqcrwOsKw)  
摘要：本文简要介绍了ICLR2020的一篇分析梯度裁剪的满分论文，主要思路是引入了更宽松普适的假设条件，在新的条件下能体现出了梯度裁剪的必要性，并且由于放松了传统的约束，因此理论结果的适用范围更广，这也就表明，梯度裁剪确实是很多场景下都适用的技巧之一。  
- [CVPR2020] [动态卷积：自适应调整卷积参数，显著提升模型表达能力 | 微软研究院AI头条](https://mp.weixin.qq.com/s/eRZ3jNuceMYKE3lEj-g1aw)  
摘要：轻量级卷积神经网络能够在较低的计算预算下运行，却也牺牲了模型性能和表达能力。对此，微软 AI 认知服务团队提出了动态卷积，与传统的静态卷积（每层单个卷积核）相比，根据注意力动态叠加多个卷积核不仅显著提升了表达能力，额外的计算成本也很小，因而对高效的 CNN 更加友好。  
动态卷积没有在每层上使用单个卷积核，而是根据注意力动态地聚合多个并行卷积核。注意力会根据输入动态地调整每个卷积核的权重，从而生成自适应的动态卷积。由于注意力是输入的函数，动态卷积不再是一个线性函数。通过注意力以非线性方式叠加卷积核具有更强的表示能力。  
动态网络引入了两部分的额外计算：注意力模型和卷积核的叠加。注意力模型计算复杂度很低，由 avg pool 和两层全卷积组成。得益于小的内核尺寸，叠加多个卷积核在计算上也非常高效。因此，动态卷积引入的额外计算是非常少的。少量的额外计算与显著的表达能力的提升使得动态卷积非常适合轻量级的神经网络。  
本文提出，解决这个问题有两个关键点。首先，限制注意力的取值将简化注意力模型的学习。注意力取值的限制将缩小多个卷积的叠加核的取值空间。文中将注意力取值限制在0与1之间，同时所有注意力的和为1。如图3所示，如果使用3个卷积核，注意力在0与1之间把叠加核限制在两个三棱锥中，注意力的和为1把叠加核进一步限制在以这三个卷积核为顶点的三角形中。对于这两个限制，softmax 是一个很自然的选择。


## 开源项目

> 注：每条内容前缀为github地址的仓库拥有者和仓库名，补全地址后为`github.com/<repo_owner>/<repo_name>`。

- [Tencent/TNN：腾讯优图开源深度学习推理框架TNN](https://github.com/Tencent/TNN)
摘要：6 月 10 日，腾讯优图实验室宣布正式开源新一代移动端深度学习推理框架 TNN。支持主流安卓、iOS 等操作系统，适配 CPU、 GPU、NPU 硬件平台。通过 ONNX 可支持 TensorFlow、PyTorch、MXNet、Caffe 等多种训练框架。  
通过 GPU 深度调优、ARM SIMD 深入汇编指令调优、低精度计算等技术手段，TNN 在性能上取得了进一步提升。引入了 INT8、 FP16、 BFP16 等多种计算低精度的支持。通过采用 8bit 整数代替 float 进行计算和存储，使模型尺寸和内存消耗均减少至 1/4，计算性能提升 50% 以上。  
此外，TNN 还引入 arm 平台 BFP16 的支持。相比浮点模型，BFP16 使模型尺寸、内存消耗减少 50%，在中低端机上的性能提升约 20%。  
- [Uber开源深度学习推理引擎Neuropod：支持调用TensorFlow、PyTorch等框架 | AI前线](https://mp.weixin.qq.com/s/2TVvbunzAPzdqyluruFaQw)  
摘要：Neuropod 是现有深度学习框架之上的一个抽象层，它提供了一个统一的接口来运行深度学习模型。Neuropod 让研究人员可以轻松地在自己选择的框架中构建模型，同时也简化了这些模型的生产化过程。  
Neuropod 支持的框架包括：TensorFlow、PyTorch、Keras 和 TorchScript，同时也可以轻松地添加新的框架。  
通过 Neuropod，应用程序只与与框架无关的 API 进行交互（下面的所有紫色部分），并且 Neuropod 将这些与框架无关的调用转换为对底层框架的调用。我们尽可能使用零拷贝操作来高效地实现这一点。


## 博文

- [NVIDIA Jetson强力工具jtop：比htop更强 | 吉浦迅科技](https://mp.weixin.qq.com/s/Rrmk-SteZnJ0M5s2fChq7w)  
摘要：Jetpack版本，Xavier工作模式，CPU数量，风扇状态，RAM，GPU状态频率，都能用jtop来查看。安装简单pip3 install jetson-stats，直接sudo jtop来执行就能查看各种信息，结合-h参数查看更多使用方法。  
- [渐入佳境的ARM：从A78浅谈ARM架构的发展 | NeuralTalk](https://mp.weixin.qq.com/s/349MAZWE4cvqt1p6q9kccw)  
摘要：作者MikeslCroom是一位十年芯片老兵，本文将对ARM的移动端新一代核心A78、X1，对其架构的发展历程做一个分析。  
- [Arm Mali GPU 四大微架构概述 | NeuralTalk](https://mp.weixin.qq.com/s/Iv-__XF6z3EtUYIka4sKAg)
摘要：本文翻译自ARM官方的GPU Architecture页面，并结合作者的经验写就而成，简单介绍了Mali的GPU的四个架构系列（Utgard、Midgard、Bifrost、Valhall）的特点。  
- [从英伟达A100 GPU说起，浅谈细粒度结构化稀疏 | 机器之心](https://mp.weixin.qq.com/s/bg8rdl8W-wohLDMYMRJlYA)  
摘要：本文要讲的是模型轻量化方法中的稀疏化等做法。以往单个权重的剪枝是随机的，导致中间很多0访存不连续，后来有了针对权重向量、单个卷积通道，整个卷积核的卷积。虽然这种结构化稀疏提升了推理速度，但是牺牲了模型精度，而原始的细粒度剪枝在推理速度上提升效果不佳。  
英伟达提出，对网络的稀疏化过程首先将权重分组，这里所举的示例以 4 个相邻的权重为一组。而后在每个权重组内限定固定个数的非零权重数目对权重进行稀疏化。在稀疏化处理后，每个权重组（同一颜色块）中都正好留下 2 个非零权重，也就是 50% 的稀疏度。  
通过对模型施加这一结构约束，获得了规整的结构，而且随机性被限制在了每个组内。随机性带来的问题可通过将同一组权重对应的数据完全装载到计算核心的寄存器上来解决，以避开随机访问外存的延迟。这一结构也允许对网络推理计算在组边界上做分割, 从而将计算分块化以最大程度利用的加速器的多核并行计算能力。  
作者建议首先训练非稀疏网络，而后对网络进行细粒度结构化剪枝，再使用学习率重卷 (learning rate rewinding) 的方式对网络进行重训练。这里的 learning rate rewinding 就是对剪枝后的网络使用原始网络的学习率安排表进行重训练。细粒度结构化稀疏的研究应用目前还处在较初期的阶段，由于其独特的硬件友好特性相信这一设计未来会被更多的加速器采用。  
- [X86 Intel CPU上的小半径中值滤波的极速优化 | GiantPandaCV](https://mp.weixin.qq.com/s/nQ-CevrSkVj4kyS3q360VA)  
摘要：本文以一个的中值滤波作为切入点，讨论了一下针对这个具体问题的优化思路，从算法逻辑到SSE、再到AVX等，速度也从最开始普通实现的8293.79ms优化到了9.32ms，具有一定参考意义。  


## [往期回顾](https://github.com/ysh329/awesome-embedded-ai)


| 2 | 0 | 2 | 0 |
|:---:|:---:|:---:|:---:|
|  | [2020-06-03](../embedded-ai-report/2020-06-03.md)  | [2020-05-15](../embedded-ai-report/2020-05-15.md) | [2020-04-26](../embedded-ai-report/2020-04-26.md) |  
| [2020-04-04](../embedded-ai-report/2020-04-04.md) | [2020-03-19](../embedded-ai-report/2020-03-19.md) | [2020-03-02](../embedded-ai-report/2020-03-02.md) | [2020-02-16](../embedded-ai-report/2020-02-16.md) |  
| [2020-01-27](../embedded-ai-report/2020-01-27.md) | [2020-01-06](../embedded-ai-report/2020-01-06.md) | [2019-12-17](../embedded-ai-report/2019-12-17.md)  |  [2019-12-02](../embedded-ai-report/2019-12-02.md) |
| 2 | 0 | 1 | 9 |  
| [2019-11-30](../embedded-ai-report/2019-11-30.md) | [2019-11-18](../embedded-ai-report/2019-11-18.md) | [2019-10-31](../embedded-ai-report/2019-10-31.md)  |  [2019-10-17](../embedded-ai-report/2019-10-17.md) |  
| [2019-10-03](../embedded-ai-report/2019-10-03.md) | [2019-09-16](../embedded-ai-report/2019-09-16.md) | [2019-08-30](../embedded-ai-report/2019-08-30.md)  |  [2019-08-15](../embedded-ai-report/2019-08-15.md) |  
| [2019-07-30](../embedded-ai-report/2019-07-30.md) | [2019-07-15](../embedded-ai-report/2019-07-15.md) | [2019-06-29](../embedded-ai-report/2019-06-29.md)  |  [2019-06-17](../embedded-ai-report/2019-06-17.md) |  
| [2019-05-30](../embedded-ai-report/2019-05-30.md) | [2019-05-15](../embedded-ai-report/2019-05-15.md) | [2019-04-27](../embedded-ai-report/2019-04-27.md)  |  [2019-04-13](../embedded-ai-report/2019-04-13.md) |  
| [2019-03-31](../embedded-ai-report/2019-03-31.md) | | |  

----

> 注：个别链接打不开，请点击文末阅读原文跳转

![wechat_qrcode](../wechat_qrcode.jpg)

> 往期回顾：见公众号主菜单【历史消息】
- WeChat: NeuralTalk  
- Editor: https://github.com/ysh329  
- Project: https://github.com/ysh329/awesome-embedded-ai  

----

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">知识共享署名-相同方式共享 4.0 通用许可协议</a>进行许可。
