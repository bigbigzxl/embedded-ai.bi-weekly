---
layout: default
---

# 嵌入式AI简报 (2020-07-02)

**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：本次

> 注：个别链接打不开，请点击文末阅读原文跳转

## 业界新闻

- [高通加大5G应用力度：发布骁龙6系手机芯片，还推出机器人和无人机5G系统 | 量子位](https://mp.weixin.qq.com/s/Bn477wuK6kz7svgwQVLDJA)  
摘要：骁龙690是高通6系列第一款5G SoC，8nm工艺打造，采用Cortex A77架构，由2个2.0GHz A77大核+6个1.7GHz A55小核组成，GPU为Adreno619L，相比上代骁龙675，骁龙690的CPU性能提升20%，GPU性能提升60%。首款支持120Hz刷新率显示的平台，支持HDR 10+，ISP单摄最高支持1.92亿像素，首次支持4K HDR（10-bit）视频拍摄，相比SDR有着更好的拍摄效果。  
AI方面，骁龙690是高通6系首款支持Qualcomm Hexagon张量加速器(HTA)、第五代AI Engine的移动平台，AI性能相比上代提升70%以上，并且AI性能的提升也让声音识别更精准。  
骁龙690的商用终端预计将于2020年下半年面市。  
此外，高通推出了一个可用于机器人和无人机的5G系统——RB5。集硬件、软件和开放工具于一体的平台套件，硬件使用基于骁龙865的Kryo 585 CPU和Adreno 650 GPU的QRB5615处理器。软件则带有用于神经处理、机器视觉、定位、特征识别和障碍物检测的SDK。  
- [骁龙875芯片组价格曝光：比骁龙865贵那么多 | 安兔兔](https://mp.weixin.qq.com/s/2whwtxkpY9NGy9EyCiSu-g)  
摘要：网传小米对高通签下的订单中，骁龙875芯片的价格高达250美元左右，小米内部高层正在讨论其下一部旗舰机型的定价问题。虽说网流不能实锤，但综合消息来看，骁龙875相比上代涨价基本确认。  
预计骁龙875将采用Kryo 685 CPU，Adreno 660 GPU、Adreno 665 VPU、Adreno 1095 DPU、高通安全处理单元（SPU250）以及Spectra 580。  
- [ARM宣布：GPU驱动支持独立更新 | 安兔兔](https://mp.weixin.qq.com/s/uAe-iZ9eIFcV1SD8XvG5FA)  
摘要：ARM声称已经为Mali GPU驱动独立更新做好了准备工作，遗憾的是官方未透露哪些GPU能获得独立更新支持。  
之前简报提到，高通骁龙865、骁龙765G等芯片支持GPU驱动独立更新。此前，GPU驱动通常会跟OTA更新捆绑在一起推送。如今它可以通过应用商店，像更新软件那样获得GPU驱动更新。小米10、小米10 Pro、Redmi K30 Pro等骁龙865机型已经支持GPU驱动独立更新。  
- [华米黄山2号戴在手腕上的RISCV NPU | 量子位](https://mp.weixin.qq.com/s/ldKnHjSq6UeVEcLtqmwfwg)  
摘要：华米承袭“黄山1号”，“黄山2号” 在RISC-V架构，进一步设计了AI本地生物数据计算NPU。采用卷积网络加速技术，提升计算性能加快识别。   
一方面，效率，健康监测中重要的心率数据，“黄山2号”将房颤识别速度较上一代提升了7倍，是市⾯上其它算法的26倍。另一方面，功耗，加入了C2协处理器之后，“黄山2号”即使在主芯片关闭状态，C2协处理器仍然可24小时处理数据使功耗下降50%，实现24小时健康监测。目前也在持续发力五大AI引擎：心率、血氧、睡眠、运动全能落地，以及将华米科技人工智能实验室正式升级为人工智能研究院。  
- [Facebook让手机2D照片秒变3D | 新智元](https://mp.weixin.qq.com/s/LG1AP_W0psDeORpM1CoQnQ)  
摘要：Facebook团队为其增加了一种算法，可以自动为输入的2D图像深度估计，改良后的技术能直接应用于任何移动设备，不限于Facebook App，且无需是双镜头摄像头拍摄的图片。有了3D图像，让照片更具生命力和真实感。现在，Facebook研究小组正在研究机器学习方法，这种方法能够对移动设备拍摄的视频进行有效深度估计，未来将适用于视频的2D转换3D。  


## 论文

- [2006.16669] [低比特离线量化EasyQuant: 比TensorFlow更友好的离线量化方式 | NeuralTalk](https://mp.weixin.qq.com/s/SwoAJ7DVPkj6cWauhzFU_g)  
标题：EasyQuant: Pose-training Quantization via Scale Optimization  
代码：https://github.com/deepglint/EasyQuant  
论文：https://arxiv.org/abs/2006.16669  
摘要：格灵深瞳DeepGlint最近开源量化算法EasyQuant。该方法通过精细搜索每层量化参数 Scale 值来实现量化精度的提升。我们的实验表明，即使是在 Int7 或更低比特的量化条件下，也能达到接近 Int8 和 Float32 的性能。基于当前 arm NEON 指令集，更低的 Int7 量化，也会带来更快的推理速度，且更容易减少溢出问题的产生。  
EasyQuant 需要的样本量极低，在用 KLD 方法得到初始量化值后，一般只需要 50 张左右的典型样本就可以完成整个量化调优过程。  
由于这是一个非训练量化（Post Training Quantization）的方法，所以它也是一个任务无关的方法，不需要考虑各种各样任务相关的 Loss 函数和数据 Label，相对于其它需要搭建完整的训练流程才能达到量化目标的方法来说，EeayQuant 体现出了应用上简单友好的特点。  
- [2006.10226v1] [Efficient Execution of Quantized Deep Learning Models: A Compiler Approach](https://arxiv.org/abs/2006.10226v1)  
论文：https://arxiv.org/abs/2006.10226v1  
摘要：Although deep learning frameworks such as TensorFlow, TFLite, MXNet, and PyTorch enable developers to quantize models with only a small drop in accuracy, they are not well suited to execute quantized models on a variety of hardware platforms. For example, TFLite is optimized to run inference on ARM CPU edge devices but it does not have efficient support for Intel CPUs and Nvidia GPUs.  
In this paper, **we address the challenges of executing quantized deep learning models on diverse hardware platforms by proposing an augmented compiler approach.** A deep learning compiler such as Apache TVM can enable the efficient execution of model from various frameworks on various targets. Many deep learning compilers today, however, are designed primarily for fp32 computation and cannot optimize a pre-quantized INT8 model.  
To address this issue, we created a new dialect called Quantized Neural Network (QNN) that extends the compiler's internal representation with a quantization context. With this quantization context, **the compiler can generate efficient code for pre-quantized models on various hardware platforms.** As implemented in Apache TVM, we observe that the QNN-augmented deep learning compiler achieves speedups of 2.35x, 2.15x, 1.35x and 1.40x on Intel Xeon Cascade Lake CPUs, Nvidia TeslaT4 GPUs, ARM Raspberry Pi3 and Pi4 respectively against well optimized fp32 execution, and comparable performance to the state-of-the-art framework-specific solutions.  
- [1801.06434] [EffNet: 继MobileNet和ShuffleNet之后的高效网络结构 | GiantPandaCV](https://mp.weixin.qq.com/s/Vcj8Z2zDmu1a7zRFCfpyew)  
标题：EffNet: AN EFFICIENT STRUCTURE FOR CONVOLUTIONAL NEURAL NETWORKS  
代码：https://github.com/andrijdavid/EffNet  
论文：https://arxiv.org/abs/1801.06434  
摘要：EffNet是在MobileNetV1,V2和ShuffleNetV1之后提出来的改进模型，指出了MobileNet和ShuffleNet的不足，由于基本的block和stride等超参数会对信息造成一定的损失，这种损失对于小型的网络尤其突出。EffNet提出就是为了解决以上问题，对shallow和narrow情况下的网络效果更好。EffNet主要有两个贡献：  
    1. 提出了EffNet Block，将深度可分离3x3卷积改进为1x3和3x1的空间可分离卷积，在两者之间添加一维maxpooling；
    2. ShuffleNet和MobileNet都选择避免处理第一层，并认为第一层的计算代价已经很低。但是EffNet认为每个优化都非常重要，如果优化了除了第一层以外的其它层，那么第一层的计算量相比之下就会比较大。实验证明使用EffNet块替换第一层能够节省30%的计算量。  


## 开源项目

> 注：每条内容前缀为github地址的仓库拥有者和仓库名，补全地址后为`github.com/<repo_owner>/<repo_name>`。

- [MindSpore新版本v0.5.0-beta官方解读 | MindSpore](https://mp.weixin.qq.com/s/YVHaVzsmGkbPxuU46dLd4w)  
摘要：MindSpore 0.5版本的Top features如下：图算融合技术、量化训练、图神经网络、分布式训练的Host+Device混合训练、可视化性能分析工具：  
这里重点介绍图算融合。该技术将算子与图层的表达进行统一，作为算子的一种新的表达方式、融合方法、编译流程。
在中间表达中，它用基础原语组成的子图来表达算子的内部计算逻辑，通过分析和优化现有网络计算图逻辑，对原有计算逻辑进行拆分、重组、融合，在中间表达中自动形成融合子图。融合子图会由AKG（Auto Kernel Generator）自动生成高性能的融合算子，且基于polyhedral实现了Auto schedule，从而实现网络整体执行时间的端到端深度优化。如在BERT中通过打开图算的使能开关可以获得14.8%的图算融合收益。  
- [CVPR2020] [APQ: Joint Search for Nerwork Architecture, Pruning and Quantization Policy]()  
代码：
- [mit-han-lab/apq: APQ: Joint Search for Network Architecture, Pruning and Quantization Policy](https://github.com/mit-han-lab/apq)  
标题：APQ: Joint Search for Nerwork Architecture, Pruning and Quantization Policy  
论文：https://arxiv.org/abs/2006.08509  
摘要：We present APQ for efficient deep learning inference on resource-constrained hardware. Unlike previous methods that separately search the neural architecture, pruning policy, and quantization policy, we optimize them in a joint manner.   
To deal with the larger design space it brings, a promising approach is to train a quantization-aware accuracy predictor to quickly get the accuracy of the quantized model and feed it to the search engine to select the best fit. However, training this quantization-aware accuracy predictor requires collecting a large number of quantized <model, accuracy> pairs, which involves quantization-aware finetuning and thus is highly time-consuming.  
To tackle this challenge, we propose to transfer the knowledge from a full-precision (i.e., fp32) accuracy predictor to the quantization-aware (i.e., int8) accuracy predictor, which greatly improves the sample efficiency. Besides, collecting the dataset for the fp32 accuracy predictor only requires to evaluate neural networks without any training cost by sampling from a pretrained once-for-all network, which is highly efficient. Extensive experiments on ImageNet demonstrate the benefits of our joint optimization approach. With the same accuracy, APQ reduces the latency/energy by 2x/1.3x over MobileNetV2+HAQ. Compared to the separate optimization approach (ProxylessNAS+AMC+HAQ), APQ achieves 2.3% higher ImageNet accuracy while reducing orders of magnitude GPU hours and CO2 emission, pushing the frontier for green AI that is environmental-friendly. The code and video are publicly available.


## 博文

- [AI芯片技术发展 | StarryHeavensAbove](https://mp.weixin.qq.com/s/JtsIxCy8ZPDCjGZThRPdzg)  
摘要：本文由壁仞科技的唐杉在Techbeat上做了一个关于AI芯片的讲座，整理的一个文字版本。非常值得一看。


> 注：个别链接打不开，请点击文末【阅读原文】跳转



## [往期回顾](https://github.com/ysh329/awesome-embedded-ai)


| 2 | 0 | 2 | 0 |
|:---:|:---:|:---:|:---:|
| - | - | - | - |
| [2020-06-17](../embedded-ai-report/2020-06-17.md) | [2020-06-03](../embedded-ai-report/2020-06-03.md)  | [2020-05-15](../embedded-ai-report/2020-05-15.md) | [2020-04-26](../embedded-ai-report/2020-04-26.md) |  
| [2020-04-04](../embedded-ai-report/2020-04-04.md) | [2020-03-19](../embedded-ai-report/2020-03-19.md) | [2020-03-02](../embedded-ai-report/2020-03-02.md) | [2020-02-16](../embedded-ai-report/2020-02-16.md) |  
| [2020-01-27](../embedded-ai-report/2020-01-27.md) | [2020-01-06](../embedded-ai-report/2020-01-06.md) | [2019-12-17](../embedded-ai-report/2019-12-17.md)  |  [2019-12-02](../embedded-ai-report/2019-12-02.md) |
| 2 | 0 | 1 | 9 |  
| [2019-11-30](../embedded-ai-report/2019-11-30.md) | [2019-11-18](../embedded-ai-report/2019-11-18.md) | [2019-10-31](../embedded-ai-report/2019-10-31.md)  |  [2019-10-17](../embedded-ai-report/2019-10-17.md) |  
| [2019-10-03](../embedded-ai-report/2019-10-03.md) | [2019-09-16](../embedded-ai-report/2019-09-16.md) | [2019-08-30](../embedded-ai-report/2019-08-30.md)  |  [2019-08-15](../embedded-ai-report/2019-08-15.md) |  
| [2019-07-30](../embedded-ai-report/2019-07-30.md) | [2019-07-15](../embedded-ai-report/2019-07-15.md) | [2019-06-29](../embedded-ai-report/2019-06-29.md)  |  [2019-06-17](../embedded-ai-report/2019-06-17.md) |  
| [2019-05-30](../embedded-ai-report/2019-05-30.md) | [2019-05-15](../embedded-ai-report/2019-05-15.md) | [2019-04-27](../embedded-ai-report/2019-04-27.md)  |  [2019-04-13](../embedded-ai-report/2019-04-13.md) |  
| [2019-03-31](../embedded-ai-report/2019-03-31.md) | | |  

----

![wechat_qrcode](../wechat_qrcode.jpg)

> 往期回顾：见公众号主菜单【历史消息】
- WeChat: NeuralTalk  
- Editor: https://github.com/ysh329  
- Project: https://github.com/ysh329/awesome-embedded-ai  

----

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">知识共享署名-相同方式共享 4.0 通用许可协议</a>进行许可。
