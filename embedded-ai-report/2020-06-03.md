---
layout: default
---

############### 嵌入式AI简报 (2020-06-03)

**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：

## 业界新闻

- [TensorFlow Lite 新功能亮相 TF DevSummit‘20 | TensorFlow](https://mp.weixin.qq.com/s/G-LyRhJwhYk1rvj6vmMFLg)  
摘要：摘一下半年内的特性要点：新模型支持MobileBERT 和 ALBERT-Lite（支持多种 NLP 任务的轻量级版本 BERT）、EfficientNet-Lite的支持；新的 TensorFlow Lite 转换器支持如DeepSpeech V2，Mask R-CNN，MobileBERT，MobileNetSSD 等；控制流方面的op支持，也支持 Keras 模型的训练时量化。  
性能方面。新的arm cpu矩阵乘法库 ruy（https://github.com/google/ruy ），以及xnnpack、Hexagon DSP、CoreML、OpenCL（安卓）等Delegate的推理支持。支持对内部事件（如算子调用）的志记录，可结合Android的System Tracing追踪性能瓶颈。详见：https://developer.android.com/topic/performance/tracing  
微控制器方面。Arduino官方提供了支持，如[5 分钟内将语音检测部署到 Arduino Nano](https://mp.weixin.qq.com/s/2rEM7T5WiaS8ft4WLYCNsg)、Cadence 宣布旗下的 Tensilica HiFi DSP 系列支持 TF Micro等。  
另外，长期计划图见：https://tensorflow.google.cn/lite/guide/roadmap 。  
- [树莓派4首发8GB版本，售价75刀，还可尝鲜64位操作系统 | 机器之心](https://mp.weixin.qq.com/s/QnaX9_JcBuF4kVSpcD-O7Q)  
摘要：树莓派 4 使用四核 64 位的 ARM Cortex-A72 处理器，具有千兆以太网，双频 802.11ac 无线网，蓝牙 5.0，两个 USB 3.0 和两个 USB 2.0，支持连接两台 4K 显示器，GPU 支持 OpenGL ES 3.x，4K 60fps HEVC 视频硬件解码等。  
基于TensorFlow Lite在同样的数据集上，树莓派4代的推理性能可达到普通TensorFlow的3～4倍，且4代推理速度超过树莓派3B+ 4 倍以上，性能直逼 Jetson Nano。配合使用 Coral USB 加速棒，速度甚至比 MBP 还快。Jetson Nano Developer Kit 官方标价 99 美元。  
- [深扒英伟达安培新架构，五大技术新招刀刀见血 | 芯东西](https://mp.weixin.qq.com/s/SXQ7eGNvVsOJUJHLEkIKUg)  
摘要：基于83页的《NVIDIA A100 Tensor Core GPU Architecture》白皮书及采访信息，
  1. 安培架构：全球最大7nm芯片，拥有542亿个晶体管，采用40GB三星HBM2，内存带宽可达到1.6 Tbps。高带宽的HBM2内存和更大、更快的缓存为增加的CUDA Core和Tensor Core提供数据；
  2. 第三代Tensor Core：处理速度更快、更灵活，TF32精度可将AI性能提升20倍；
  3. 结构化稀疏：进一步将AI推理性能提升2倍；
  4. 多实例GPU：每个GPU可分成7个并发实例，优化GPU利用率；
  5. 第三代NVLink和NVSwitch：高效可扩展，带宽较上一代提升2倍有余。
本文将会提炼安培GPU架构在计算和内存层次结构的关键创新与改进，深度解析这一全新架构怎样实现NVIDIA迄今为止最大的性能飞跃。  
- [面向下一代Web的深度学习编译：WebAssembly和WebGPU的TVM初探 | 知乎](https://zhuanlan.zhihu.com/p/141008345)  
摘要：目前有很多框架如tf.js, onnx.js在支持浏览器部署，但往往浏览器端无法充分利用GPU资源。即使WebGL可通过图形渲染的方式去访问GPU，但依然无法引入shared memory、generic storage buffer的概念去优化程序（虽然OpenGL新版本部分解决，但目前WebGL依然是基于旧的OpenGL标准）。  
最近Web端两个重要的新元素 -- WebAssembly 和 WebGPU 给了解决浏览器端机器学习一个新的希望。WebGPU是下一代互联网的图形学接口，目前已经进入了实现阶段，主要浏览器的nightly版本已经加入了WebGPU的支持。从API上，WebGPU支持了compute shader，使得更加极致优化浏览器端的算子成为可能。  
为了探索这个可能性，TVM社区最近加入了WebAssembly和WebGPU后端的支持。通过已有的架构生成嵌入WebGPU compute shader的wasm模块。在Chrome预览版本上的测试结果展示了很大的潜力 -- tvm生成的WebGPU模块在MacOS上可以获得和直接本地运行native metal几乎一样的效率。


## 论文


- [一篇关于深度学习编译器架构的综述论文 | 知乎](https://zhuanlan.zhihu.com/p/139552817)  
标题：The Deep Learning Compiler: A Comprehensive Survey  
链接：https://arxiv.org/abs/2002.03794  
摘要：目前还都没有全面分析深度学习编译器这种独特设计架构。本文详细剖析常用的设计思想，对现有的深度学习编译器进行全面总结，重点是面向深度学习的多级中间表示（IR）以及前后端的优化。具体来说，作者从各个方面对现有编译器做全面比较，对多级IR的设计进行了详细分析，并介绍了常用的优化技术。最后，文章强调对今后编译器潜在研究方向的一些见解。基本上这是深度学习编译器设计体系结构（不是硬件方面）的第一个综述。  

## 开源项目

> 注：每条内容前缀为github地址的仓库拥有者和仓库名，补全地址后为`github.com/<repo_owner>/<repo_name>`。

- [mindspore-ai/mindspore:新版本v0.3.0-alpha发布！差分隐私、二阶优化、Pytorch模型支持等六大杀器来袭！ | MindSpore](https://mp.weixin.qq.com/s/5dvKTKgXt1Rl6hdKwWPr2A)  
摘要：暨v0.2.0-alpha版本发布1个月，v0.3.0-alpha按时推出。本次特性包含不限于：
  1. 差分隐私，MindSpore的安全组件提供了差分隐私模块Differential-Privacy，提供支持基于高斯机制的差分隐私优化器（SGD、Momentum），同时还提供差分隐私预算监测器，方便观察差分隐私效果变化。文档：https://www.mindspore.cn/tutorial/zh-CN/0.3.0-alpha/advanced_use/differential_privacy.html；  
  2. 二阶优化，ResNet达到精度（0.759）仅用了42个迭代（epoch），比同软硬件环境的一阶优化快了近1倍（一阶优化使用了81epoch达到目标精度）。文档：https://gitee.com/mindspore/mindspore/tree/r0.3/example/resnet50_imagenet2012_THOR；
  3. 支持Pytorch模型转换。  
除了以上，还新增支持了DeepFM、DeepLabV3、Wide&Deep等新的模型，修复了一些关键bug，增添了网络迁移教程、自定义算子教程等。  
- [matazure/mtensor：同时支持C++和cuda延迟计算的异构计算库 | 极市平台](https://mp.weixin.qq.com/s/3Z_ZvJ7p2z-TqfHwtCp_5Q)  
摘要：mtensor是一个tensor计算库，主要用于多维数组及其计算。其可以结构化高效地在CPU/GPU上实现遍历、滤波、转换等多种操作。也便于数据在CPU与GPU之间的传输交互。mtensor主要特点是延迟计算。  
延迟计算有多种实现方式，最为常见的是eigen所采用的模板表达式。但该种方式每实现一种新的运算就要实现一个完整的模板表达式class且过程繁琐，不易拓展新运算。  
mtensor自研的基于闭包算子的lambda tensor是一种更为通用简洁的延迟计算实现。此外，目前绝大部分支持延迟计算的库都没支持cuda，而对于gpu这种计算能里远强于内存带宽的设备来说延迟计算尤为重要。cuda 9版本以来，cuda C++逐渐完善了对 C++11 和 C++14 的支持，使得cuda的延迟计算可以得到简洁的实现。  

- [JacopoMangiavacchi/MNIST-CoreML-Training:Training MNIST with CoreML on Device](https://mp.weixin.qq.com/s/JNYjA7hctJlkwW6prUEEvQ)  
摘要：在这篇文章中，作者介绍了如何使用 MNIST 数据集部署一个图像分类模型，值得注意的是，这个 Core ML 模型是在 iOS 设备上直接训练的，而无需提前在其他 ML 框架中进行训练。


## 博文

- [Tengine armv8.2 with ncnn serializer | 知乎](https://zhuanlan.zhihu.com/p/140968716)  
摘要：本文将会为大家评估armv8.2的FP16的FMLA指令和Int8的SDOT指令的计算能力，并结合Tengine开源版本，采用Hybrid-FP16策略，让计算核心模块（im2col+sgemm）采用FP16的前提下，使用 Tengine 来跑 NCNN 模型，解锁更强性能。  
- [TensorFlow Lite的 GPU 委托代理（Delegate）是什么 | NeuralTalk](https://mp.weixin.qq.com/s/jxW7mcysdl-CsUHb5oYVVQ)  
摘要：目前TFLite的 Delegate支持了 OPENCL 、Hexagon DSP、XNNPACK等。本文从TensorFlow Lite的文档出发并结合作者思考，为大家尽可能通俗地讲解什么是TensorFlow Lite 的 Delegate 。  
- [旷视 MegEngine 推理性能极致优化之综述篇 | 旷视研究院](https://mp.weixin.qq.com/s/FKIibmuNrpSnR7tXlXs8Pg)  
摘要：MegEngine「训练推理一体化」的独特范式，通过静态图优化保证模型精度与训练时一致，无缝导入推理侧，再借助工业验证的高效卷积优化技术，打造深度学习推理侧极致加速方案，实现当前业界最快运行速度。  
本文从推理侧的数据排布（Inference Layout）讲起，接着介绍MegEngine的Im2col+MatMul、Winograd、Fast-Run工程优化实践。经典的轻量卷积神经网络实验表明，经过MegEngine加速，ResNet18和ResNet50最高加速比可达2x以上，ShuffleNet V2和MobileNet V2执行效率也得到显著提升，实现了业界当前最佳推理性能。  
- [案例分享 | 怎样基于 tfjs-node 构建一个高阶前端机器学习平台 | TensorFlow](https://mp.weixin.qq.com/s/KZZPh9N2tFdWEkfW_ODlHw)  
摘要：为了解决这些问题，推动前端智能化发展，alibaba 开发了 pipcook。pipcook 使用对前端友好的 JS 环境，基于 TensorFlow.js 框架作为底层算法能力并且针对前端业务场景包装了相应算法，从而让前端工程师可以快速简单的运用起机器学习的能力。  
此篇文章将主要讨论 pipcook 是如何和 TensorFlow.js 结合的，重点探讨一下如何利用 tfjs-node 的底层模型和计算能力搭建一个高阶机器学习流水线，您可以在这里参考 pipcook (https://github.com/alibaba/pipcook) 的更多信息。  
- [深入理解C11/C++11内存模型 | Linux阅码场](https://mp.weixin.qq.com/s/YV2lY9uUbWUq_HWh8z_hSw)  
摘要：现代计算机体系结构上，CPU执行指令的速度远远大于CPU访问内存的速度，于是引入Cache机制来加速内存访问速度。除了Cache以外，分支预测和指令预取也在很大程度上提升了CPU的执行速度。随着SMP的出现，多线程编程模型被广泛应用，在多线程模型下对共享变量的访问变成了一个复杂的问题。于是我们有必要了解一下内存模型，这是多处理器架构下并发编程里必须掌握的一个基础概念。  


## [往期回顾](https://github.com/ysh329/awesome-embedded-ai)


| 2 | 0 | 2 | 0 |
|:---:|:---:|:---:|:---:|
|  |  | [2020-05-15](../embedded-ai-report/2020-05-15.md) | [2020-04-26](../embedded-ai-report/2020-04-26.md) |  
| [2020-04-04](../embedded-ai-report/2020-04-04.md) | [2020-03-19](../embedded-ai-report/2020-03-19.md) | [2020-03-02](../embedded-ai-report/2020-03-02.md) | [2020-02-16](../embedded-ai-report/2020-02-16.md) |  
| [2020-01-27](../embedded-ai-report/2020-01-27.md) | [2020-01-06](../embedded-ai-report/2020-01-06.md) | [2019-12-17](../embedded-ai-report/2019-12-17.md)  |  [2019-12-02](../embedded-ai-report/2019-12-02.md) |
| 2 | 0 | 1 | 9 |  
| [2019-11-30](../embedded-ai-report/2019-11-30.md) | [2019-11-18](../embedded-ai-report/2019-11-18.md) | [2019-10-31](../embedded-ai-report/2019-10-31.md)  |  [2019-10-17](../embedded-ai-report/2019-10-17.md) |  
| [2019-10-03](../embedded-ai-report/2019-10-03.md) | [2019-09-16](../embedded-ai-report/2019-09-16.md) | [2019-08-30](../embedded-ai-report/2019-08-30.md)  |  [2019-08-15](../embedded-ai-report/2019-08-15.md) |  
| [2019-07-30](../embedded-ai-report/2019-07-30.md) | [2019-07-15](../embedded-ai-report/2019-07-15.md) | [2019-06-29](../embedded-ai-report/2019-06-29.md)  |  [2019-06-17](../embedded-ai-report/2019-06-17.md) |  
| [2019-05-30](../embedded-ai-report/2019-05-30.md) | [2019-05-15](../embedded-ai-report/2019-05-15.md) | [2019-04-27](../embedded-ai-report/2019-04-27.md)  |  [2019-04-13](../embedded-ai-report/2019-04-13.md) |  
| [2019-03-31](../embedded-ai-report/2019-03-31.md) | | |  

----

![wechat_qrcode](../wechat_qrcode.jpg)

> 往期回顾：见公众号主菜单【历史消息】
- WeChat: NeuralTalk  
- Editor: https://github.com/ysh329  
- Project: https://github.com/ysh329/awesome-embedded-ai  

----

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">知识共享署名-相同方式共享 4.0 通用许可协议</a>进行许可。
