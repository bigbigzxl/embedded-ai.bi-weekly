---
layout: default
---

# 嵌入式AI简报 (2020-06-03)

**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：

抖-音AI芯片计划曝光的相关文章被撤下，

[OPPO首次公开了关于自研芯片的“马里亚纳计划”](https://mp.weixin.qq.com/s/D_SCX0sEObSsWp1JoCHU4A)，从供应商方面如紫光展锐和联发科方面招募了不少工程师等高管，

华为现在还希望购买联发科中高端5G移动芯片。之前，华为高端手机上只使用自家芯片。

一位知情人士称：“对于当前所面临的情况，华为之前已经预测到了。去年，华为就开始将更多的中低端移动芯片项目分配给联发科。今年，华为已经成为联发科中端5G移动芯片的关键客户之一。”
华为要求的数量比过去几年通常的采购数量高出300%。

与此同时，华为还在寻求深化与紫光展锐的合作。紫光展锐是中国内地一家移动芯片开发商，主要客户为规模相对较小的设备制造商，主要支持面向新兴市场的入门级产品和设备。


## 业界新闻

- [https://mp.weixin.qq.com/s/2LbXtfGdax-Pvdx_NoFsaQ

ARM官方正式发布了旗下的Cortex-A78以及Cortex-X1架构，前者定位是大核心，后者则是超大核。
由于架构和A76/A77同属于Austin微架构，所以实际上A78的IPC性能仅提升了7%，功耗降低了4%，内核小了5%，四核簇面积的缩小了15%。
Cortex-X系列是ARM全新推出的高性能核心架构，首款产品就是Cortex-X1，
相比A78来说，Cortex-X1解码带宽从4路提升到5路，增加了25%，NEON浮点从2条128b提升到了4条128b，相当于浮点性能翻倍。

而在缓存方面，Cortex-X1的L1缓存可达64KB，L2缓存1MB，L3缓存可达8MB，达到了Cortex-A78的两倍。



在核心搭配方面，Cortex-X1可以跟Cortex-A78、Cortex-A55来组建三丛集的架构，也就是一颗X1、三颗A78搭配四颗A55，兼顾超大核大核以及小核。

在ARM的官方文档中，这种架构可以搭配1MB L2、8MB L3，组成一套DynamIQ集群。



如果是4个Cortex-A78、搭配4MB L3缓存，其性能比前代升20%，同时核心面积降低15%。而如果是个Cortex-X1、3个Cortex-A78搭配8MB L3缓存的话，虽然核心面积会增加15%，但峰值性能提升了30%。

在发布Cortex-A78以及Cortex-X1 CPU架构的同时，ARM还发布了全新的旗舰GPU架构Mali-G78以及定位次旗舰的Mali-G68。
Mali-G78可以看作是Mali-G77的续作，依然基于全新的Valhall图形架构，最高可配置24颗核心（G77仅为16核心）。
Mali-G78在架构方面也有调整，原本G77只有一个全局时钟域，而G78则改为两级结构，实现了异步时钟域，也就是上层共享GPU模块与实际着色器核心频率的分离。
这么做的好处就是GPU核心的频率可以和GPU其它部分的频率不同，实现动态调整，从而降低功耗、提高能效。
Mali-G78重构了FMA引擎，包括新的乘法架构、新的加法架构、FP32/FP16浮点，执行效率更高，号称可以节省30％的功耗。
而Mali-G68的架构实际上和Mali-G78是一样的，但最多只能配置6个核心。简单来说，配置1-6个核心就叫Mali-G68，配置7-24个核心就叫你Mali-G78，和Mali-G57/G77类似。




- [TensorFlow Lite 新功能亮相 TF DevSummit‘20 | TensorFlow](https://mp.weixin.qq.com/s/G-LyRhJwhYk1rvj6vmMFLg)  
摘要：摘一下半年内的特性要点：新模型支持MobileBERT 和 ALBERT-Lite（支持多种 NLP 任务的轻量级版本 BERT）、EfficientNet-Lite的支持；新的 TensorFlow Lite 转换器支持如DeepSpeech V2，Mask R-CNN，MobileBERT，MobileNetSSD 等；控制流方面的op支持，也支持 Keras 模型的训练时量化。  
性能方面。新的arm cpu矩阵乘法库 ruy（https://github.com/google/ruy ），以及xnnpack、Hexagon DSP、CoreML、OpenCL（安卓）等Delegate的推理支持。支持对内部事件（如算子调用）的志记录，可结合Android的System Tracing追踪性能瓶颈。详见：https://developer.android.com/topic/performance/tracing  
微控制器方面。Arduino官方提供了支持，如[5 分钟内将语音检测部署到 Arduino Nano](https://mp.weixin.qq.com/s/2rEM7T5WiaS8ft4WLYCNsg)、Cadence 宣布旗下的 Tensilica HiFi DSP 系列支持 TF Micro等。  
另外，长期计划图见：https://tensorflow.google.cn/lite/guide/roadmap 。  
- [树莓派4首发8GB版本，售价75刀，还可尝鲜64位操作系统 | 机器之心](https://mp.weixin.qq.com/s/QnaX9_JcBuF4kVSpcD-O7Q)  
摘要：树莓派 4 使用四核 64 位的 ARM Cortex-A72 处理器，具有千兆以太网，双频 802.11ac 无线网，蓝牙 5.0，两个 USB 3.0 和两个 USB 2.0，支持连接两台 4K 显示器，GPU 支持 OpenGL ES 3.x，4K 60fps HEVC 视频硬件解码等。  
基于TensorFlow Lite在同样的数据集上，树莓派4代的推理性能可达到普通TensorFlow的3～4倍，且4代推理速度超过树莓派3B+ 4 倍以上，性能直逼 Jetson Nano。配合使用 Coral USB 加速棒，速度甚至比 MBP 还快。Jetson Nano Developer Kit 官方标价 99 美元。  
- [深扒英伟达安培新架构，五大技术新招刀刀见血 | 芯东西](https://mp.weixin.qq.com/s/SXQ7eGNvVsOJUJHLEkIKUg)  
摘要：基于83页的《NVIDIA A100 Tensor Core GPU Architecture》白皮书及采访信息，本文将会提炼安培GPU架构在计算和内存层次结构的关键创新与改进，深度解析这一全新架构怎样实现NVIDIA迄今为止最大的性能飞跃。其中特向包括不限于：  
  1. 安培架构：全球最大7nm芯片，拥有542亿个晶体管，采用40GB三星HBM2，内存带宽可达到1.6 Tbps。高带宽的HBM2内存和更大、更快的缓存为增加的CUDA Core和Tensor Core提供数据；
  2. 第三代Tensor Core：处理速度更快、更灵活，TF32精度可将AI性能提升20倍；
  3. 结构化稀疏：进一步将AI推理性能提升2倍；
  4. 多实例GPU：每个GPU可分成7个并发实例，优化GPU利用率；
  5. 第三代NVLink和NVSwitch：高效可扩展，带宽较上一代提升2倍有余。    



## 论文

- [为了不让GPU等CPU，谷歌提出“数据回波”榨干GPU空闲时间，训练速度提升3倍多 | 量子位](https://mp.weixin.qq.com/s/QLf7x5l1aok4pJ5xfZcdtw)  
链接：https://arxiv.org/abs/1907.05550
博客：https://ai.googleblog.com/2020/05/speeding-up-neural-network-training.html
摘要：训练时GPU或者是更快的AI加速器实际上在等上游如图像预处理等的计算，且由于上游和加速器的下游的串行流水排布，导致AI加速器的使用率并非100%。本文作者提出，加速器空置50%情况下，预处理batch的第一个优化步骤之后，可重复利用该batch再进行一次训练。如果重复数据与新数据一样有用，既降低磁盘IO，训练效率也提高一倍。
将数据复制到训练管道中某个位置的随机缓冲区中，无论哪个阶段产生瓶颈，都可将缓存数据插入任意位置。数据回波在样本级别对数据Shuffle，而batch回波则对重复批次的序列做Shuffle。另外还可以在数据扩充之前插入缓冲区，以使重复数据的每个副本略有不同，因此不是简单机械重复，而是更接近一个新样本。  
随着如TPU等AI专用加速器的性能提升，即也表示和通用处理器的差距会越来越大，Google期望数据回波和类似策略将成为神经网络培训工具包中越来越重要的一部分。  
- [CVPR2020] [纪荣嵘教授团队提出基于高秩特征图的滤波器剪枝方法：不引入约束，浮点运算和参数量显著减少](https://mp.weixin.qq.com/s/tAF-16XK8a8DusMxEDgDtQ)  
链接：https://arxiv.org/abs/2002.10179
项目：https://github.com/lmbxmu/HRank
摘要：HRank 的灵感来自于这样一个发现：无论 CNN 接收的图像批数是多少，由单个滤波器生成的多个特征图的平均秩总是相同的。在 HRank 的基础上，研究者还提出了一种针对低秩特征图所对应滤波器进行剪枝的算法。剪枝的原理是低秩特征图包含的信息较少，因此剪枝后的结果可以很容易地再现。
算法流程：首先，计算某个特征图的秩的平均，对这些秩进行降序排列；然后，确定某一卷积层的待保留滤波器数量和待修剪滤波器数量，其实也就是确定卷积层对应的压缩率；最后，根据计算得到的秩，从所有滤波器中筛选出秩较高的那些，从而建立剪枝后的模型。  
在 CIFAR-10 和 ImageNet 两个数据集上进行了实验，使用 VGGNet、GoogLeNet、ResNet 和 DenseNet 作为 Baseline 模型，以测试本文方法对于各种结构的适用性，实验结果优秀。  

## 开源项目

> 注：每条内容前缀为github地址的仓库拥有者和仓库名，补全地址后为`github.com/<repo_owner>/<repo_name>`。

- [mindspore-ai/mindspore:新版本v0.3.0-alpha发布！差分隐私、二阶优化、Pytorch模型支持等六大杀器来袭！ | MindSpore](https://mp.weixin.qq.com/s/5dvKTKgXt1Rl6hdKwWPr2A)  
摘要：暨v0.2.0-alpha版本发布1个月，v0.3.0-alpha按时推出。本次特性包含不限于：
  1. 差分隐私，MindSpore的安全组件提供了差分隐私模块Differential-Privacy，提供支持基于高斯机制的差分隐私优化器（SGD、Momentum），同时还提供差分隐私预算监测器，方便观察差分隐私效果变化。文档：https://www.mindspore.cn/tutorial/zh-CN/0.3.0-alpha/advanced_use/differential_privacy.html；  
  2. 二阶优化，ResNet达到精度（0.759）仅用了42个迭代（epoch），比同软硬件环境的一阶优化快了近1倍（一阶优化使用了81epoch达到目标精度）。文档：https://gitee.com/mindspore/mindspore/tree/r0.3/example/resnet50_imagenet2012_THOR；
  3. 支持Pytorch模型转换。  
除了以上，还新增支持了DeepFM、DeepLabV3、Wide&Deep等新的模型，修复了一些关键bug，增添了网络迁移教程、自定义算子教程等。  
- [matazure/mtensor：同时支持C++和cuda延迟计算的异构计算库 | 极市平台](https://mp.weixin.qq.com/s/3Z_ZvJ7p2z-TqfHwtCp_5Q)  
摘要：mtensor是一个tensor计算库，主要用于多维数组及其计算。其可以结构化高效地在CPU/GPU上实现遍历、滤波、转换等多种操作。也便于数据在CPU与GPU之间的传输交互。mtensor主要特点是延迟计算。  
延迟计算有多种实现方式，最为常见的是eigen所采用的模板表达式。但该种方式每实现一种新的运算就要实现一个完整的模板表达式class且过程繁琐，不易拓展新运算。  
mtensor自研的基于闭包算子的lambda tensor是一种更为通用简洁的延迟计算实现。此外，目前绝大部分支持延迟计算的库都没支持cuda，而对于gpu这种计算能里远强于内存带宽的设备来说延迟计算尤为重要。cuda 9版本以来，cuda C++逐渐完善了对 C++11 和 C++14 的支持，使得cuda的延迟计算可以得到简洁的实现。  

- [JacopoMangiavacchi/MNIST-CoreML-Training:Training MNIST with CoreML on Device](https://mp.weixin.qq.com/s/JNYjA7hctJlkwW6prUEEvQ)  
摘要：在这篇文章中，作者介绍了如何使用 MNIST 数据集部署一个图像分类模型，值得注意的是，这个 Core ML 模型是在 iOS 设备上直接训练的，而无需提前在其他 ML 框架中进行训练。


## 博文

- [Tengine armv8.2 with ncnn serializer | 知乎](https://zhuanlan.zhihu.com/p/140968716)  
摘要：本文将会为大家评估armv8.2的FP16的FMLA指令和Int8的SDOT指令的计算能力，并结合Tengine开源版本，采用Hybrid-FP16策略，让计算核心模块（im2col+sgemm）采用FP16的前提下，使用 Tengine 来跑 NCNN 模型，解锁更强性能。  
- [TensorFlow Lite的 GPU 委托代理（Delegate）是什么 | NeuralTalk](https://mp.weixin.qq.com/s/jxW7mcysdl-CsUHb5oYVVQ)  
摘要：目前TFLite的 Delegate支持了 OPENCL 、Hexagon DSP、XNNPACK等。本文从TensorFlow Lite的文档出发并结合作者思考，为大家尽可能通俗地讲解什么是TensorFlow Lite 的 Delegate 。  
- [旷视 MegEngine 推理性能极致优化之综述篇 | 旷视研究院](https://mp.weixin.qq.com/s/FKIibmuNrpSnR7tXlXs8Pg)  
摘要：MegEngine「训练推理一体化」的独特范式，通过静态图优化保证模型精度与训练时一致，无缝导入推理侧，再借助工业验证的高效卷积优化技术，打造深度学习推理侧极致加速方案，实现当前业界最快运行速度。  
本文从推理侧的数据排布（Inference Layout）讲起，接着介绍MegEngine的Im2col+MatMul、Winograd、Fast-Run工程优化实践。经典的轻量卷积神经网络实验表明，经过MegEngine加速，ResNet18和ResNet50最高加速比可达2x以上，ShuffleNet V2和MobileNet V2执行效率也得到显著提升，实现了业界当前最佳推理性能。  
- [面向下一代Web的深度学习编译：WebAssembly和WebGPU的TVM初探 | 知乎](https://zhuanlan.zhihu.com/p/141008345)  
摘要：目前有很多框架如tf.js, onnx.js在支持浏览器部署，但往往浏览器端无法充分利用GPU资源。即使WebGL可通过图形渲染的方式去访问GPU，但依然无法引入shared memory、generic storage buffer的概念去优化程序（虽然OpenGL新版本部分解决，但目前WebGL依然是基于旧的OpenGL标准）。  
最近Web端两个重要的新元素 -- WebAssembly 和 WebGPU 给了解决浏览器端机器学习一个新的希望。WebGPU是下一代互联网的图形学接口，目前已经进入了实现阶段，主要浏览器的nightly版本已经加入了WebGPU的支持。从API上，WebGPU支持了compute shader，使得更加极致优化浏览器端的算子成为可能。  
为了探索这个可能性，TVM社区最近加入了WebAssembly和WebGPU后端的支持。通过已有的架构生成嵌入WebGPU compute shader的wasm模块。在Chrome预览版本上的测试结果展示了很大的潜力 -- tvm生成的WebGPU模块在MacOS上可以获得和直接本地运行native metal几乎一样的效率。  
- [深入理解C11/C++11内存模型 | Linux阅码场](https://mp.weixin.qq.com/s/YV2lY9uUbWUq_HWh8z_hSw)  
摘要：现代计算机体系结构上，CPU执行指令的速度远远大于CPU访问内存的速度，于是引入Cache机制来加速内存访问速度。除了Cache以外，分支预测和指令预取也在很大程度上提升了CPU的执行速度。随着SMP的出现，多线程编程模型被广泛应用，在多线程模型下对共享变量的访问变成了一个复杂的问题。于是我们有必要了解一下内存模型，这是多处理器架构下并发编程里必须掌握的一个基础概念。  


## [往期回顾](https://github.com/ysh329/awesome-embedded-ai)


| 2 | 0 | 2 | 0 |
|:---:|:---:|:---:|:---:|
|  |  | [2020-05-15](../embedded-ai-report/2020-05-15.md) | [2020-04-26](../embedded-ai-report/2020-04-26.md) |  
| [2020-04-04](../embedded-ai-report/2020-04-04.md) | [2020-03-19](../embedded-ai-report/2020-03-19.md) | [2020-03-02](../embedded-ai-report/2020-03-02.md) | [2020-02-16](../embedded-ai-report/2020-02-16.md) |  
| [2020-01-27](../embedded-ai-report/2020-01-27.md) | [2020-01-06](../embedded-ai-report/2020-01-06.md) | [2019-12-17](../embedded-ai-report/2019-12-17.md)  |  [2019-12-02](../embedded-ai-report/2019-12-02.md) |
| 2 | 0 | 1 | 9 |  
| [2019-11-30](../embedded-ai-report/2019-11-30.md) | [2019-11-18](../embedded-ai-report/2019-11-18.md) | [2019-10-31](../embedded-ai-report/2019-10-31.md)  |  [2019-10-17](../embedded-ai-report/2019-10-17.md) |  
| [2019-10-03](../embedded-ai-report/2019-10-03.md) | [2019-09-16](../embedded-ai-report/2019-09-16.md) | [2019-08-30](../embedded-ai-report/2019-08-30.md)  |  [2019-08-15](../embedded-ai-report/2019-08-15.md) |  
| [2019-07-30](../embedded-ai-report/2019-07-30.md) | [2019-07-15](../embedded-ai-report/2019-07-15.md) | [2019-06-29](../embedded-ai-report/2019-06-29.md)  |  [2019-06-17](../embedded-ai-report/2019-06-17.md) |  
| [2019-05-30](../embedded-ai-report/2019-05-30.md) | [2019-05-15](../embedded-ai-report/2019-05-15.md) | [2019-04-27](../embedded-ai-report/2019-04-27.md)  |  [2019-04-13](../embedded-ai-report/2019-04-13.md) |  
| [2019-03-31](../embedded-ai-report/2019-03-31.md) | | |  

----

![wechat_qrcode](../wechat_qrcode.jpg)

> 往期回顾：见公众号主菜单【历史消息】
- WeChat: NeuralTalk  
- Editor: https://github.com/ysh329  
- Project: https://github.com/ysh329/awesome-embedded-ai  

----

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">知识共享署名-相同方式共享 4.0 通用许可协议</a>进行许可。
